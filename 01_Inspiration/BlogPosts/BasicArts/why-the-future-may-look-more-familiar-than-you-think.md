---
title: Why the future may look more familiar than you think - Basic Arts
source_url: https://basicarts.org/why-the-future-may-look-more-familiar-than-you-think/
date_published: 2018-08-31
notion_page_id: ""
migration_date: 2025-11-11
status: extracted
---
_This piece originally appeared in[Arc Digital.](https://arcdigital.media/why-
the-future-may-look-more-familiar-than-you-think-8111ed9f044a)_

The tech revolution has claimed many scalps. Film cameras, video stores,
landlines, and adult magazines — these are just a few of the technologies that
have been thrown into obsolescence by the digitalization of everything.

Less noticeably, technological progress has also slowed for a product that
would seem to thrive in our current age, but has found itself in a gradual
decline: the personal computer.

It’s not our usage of PCs that has declined. They remain essential to our
functioning, and despite the advent of tablets and smartphones, PCs have
resolutely hung on as our work device of choice. What has declined, however is
their sales. Since 2010 — the peak year for computer sales — rates have
gradually eased. They are occasionally boosted by increased adoptions in
emerging economies, but on the whole, rates have settled into an apparent
permanent stagnation.

At first glance, this seems surprising, given that computers are 1) more
central to our lives than ever, and 2) surely increasing their penetration as
computer illiterate generations die off and “digital natives” come of age.
However, those cannot offset the withering of the most important driver of the
industry: upgrades.

People are holding on to their computers for longer. It’s not a coincidence
that this is happening now. In (roughly) 2010 a crucial barrier was passed—for
the first time ever, the available hardware adequately matched the tasks that
were asked of it. Computers booted up with minimal fuss. They opened and saved
documents instantly, and crashed increasingly rarely. Internet-based programs
went from _tolerably slow_ to _satisfyingly quick_. And the tag team of USB
drives and good Wifi seemingly put an end to the search for better port
technology.

In short, computers had become _good enough._

While manufacturers continue to make them supposedly better, not enough people
care as their current model remains functional. The market, finally, has been
satisfied.

It is this concept of _good enough_ that I want to explore in this piece; the
idea that in many product sectors the primary limiting factor for innovation
and continued progress is not, as many people think, technical limitation. It
is instead the fact that eventually technologies reach a point where they’re
_fine_ , prompting the market to relax and continue rolling in that groove
indefinitely without major innovations.

The reason this is important is because it is so rarely considered by tech
evangelists, entrepreneurs, and others who predict the state of future
technology. They tend to assume that whatever is both _possible_ and _better_
is inevitable. This belief partly motivated the assumption behind the now-
collapsed [Moore’s Law](https://en.wikipedia.org/wiki/Moore%27s_law), and
still shapes our expectation of what the world will look like five, 10, 20
years from now.

To the market, however, _possible_ and _better_ count for very little if the
status quo is basically _good enough._ As such, many grand visions rarely come
to fruition. People aren’t sufficiently motivated to support them. We just
don’t need things to be better.

To illustrate this idea, think of window blinds technology. Windows blinds
have, on the whole, remained unchanged for the last 60 years. The prevailing
“tech” is that string-pulling mechanism where you drag it one way to release,
and the other way to lock. It’s not as if nobody has ever tried to make a
_better_ or _more advanced_ window blind. They have, and you can buy them. You
may have even stayed in a hotel where the blinds raise electronically to wake
you up at a pre-determined time — pretty nifty. However, these innovative
blinds have never really caught on with the general population. For most
people, the string blinds are just fine, and we can reasonably expect that in
100 years time people will still be fiddling with them even though this is in
fact a labor they could easily eradicate.

Similar technological slackening can — if you look carefully — be observed
everywhere; even in the most cutting-edge of technologies.

Smartphones are an obvious contemporary example. Although built-in
obsolescence as well as physical wear and tear keep the upgrade cycle
reasonably steady, it would be fair to say that there has been little
meaningful change in the technology since the 2010 “satisfaction point” for
computing. I, for one, very happily use an iPhone SE, a phone with the same
chassis as the 2012 iPhone 5 — a phone which itself could do everything we ask
from our devices today (other than run the latest software updates).

Can we conclude therefore that the smartphone is _good enough_ , and that
there is nowhere particularly desirable to take it to from here? Quite
possibly — particularly if we consider that many attempts to innovate
smartphones have actually resulted in _backward_ steps in terms of
functionality and convenience.

Take the development of unlocking systems. First we used a code — which
certainly hit the _good enough_ threshold. Then we had fingerprint unlock,
which represented a marginal improvement on _good enough._ People happily went
along with this because (crucially) it required no change in their buying
behavior, since they were going to upgrade anyway. And now, in a desperate
attempt to keep this stagnant category moving, we have face unlock — a system
that is _less_ convenient than fingerprint unlock because you have to move the
phone to a certain position in order to use it.

The same peril might well await more radical proposals to improve smartphone
technology. At the moment, there is a great deal of excitement around “voice,”
leading to speculation that instead of a phone we could simply have an
earpiece/contact lens/chip-in-the-brain to which we could talk so that we
might more smoothly perform our smartphone tasks. But would such an innovation
be better than the status quo? Surely we want don’t want to verbalize our
every personal message or embarrassing search throughout the day. The only
silent way to perform such tasks is typing — which needs a keyboard and
therefore a smartphone.

Even if we speculate about “brain-scanning” sci-fi alternatives, it’s not
clear they would really be much more useful than good old written text.
Massive visual changes that offer little functional progress are rarely
successful (thank you, Google Glass, for illustrating that one). Even in
fantastical speculation, it’s pretty hard to improve meaningfully upon what
we’ve got. Not only should we expect people in the future to be fiddling with
window blinds, but smartphones too.

Of course, examples such as these are fairly trivial so far as innovation
goes, and do not dominate contemporary discourse on the subject. We are more
moved by dramatic innovations such as self-driving cars or the ever-nebulous
AI and its associated automation. Unfortunately for the big dreamers, however,
the effect of _good enough_ is even more pronounced in technologies where the
status quo is fine, and to reach that future ideal, the public would have to
pass through a series of intermediary (and possibly less-than-desirable) beta-
version stepping stones.

If I were to offer you 20 minute trans-Atlantic travel, you’d probably take
it. Who wouldn’t? But that’s not the real question. You can’t just click your
fingers to get such an outcome; you have to edge your way there incrementally,
and each incremental step has to be market-friendly. The real question
therefore becomes whether you want the Concorde. Say yes to that, and we’ll
then move on to the next question. But apparently we decided that six-hour
crossings were a fair deal, and that the juice of four hours just wasn’t worth
the squeeze. Thus the Concorde failed, and with it the foreseeable possibility
of an eventual 20-minute crossing. This is the dilemma that faces every
paradigm-shifting vision: It’s not enough for people to want the dream, they
have to want the crap proto-versions of it too.

So while it would clearly be possible to have self-driving cars everywhere,
and perhaps even preferable to what we have now (though that is debatable if
you work as a driver), the fact remains that it’s a long road to get to that
point. Whether those intermediary compromises would survive their collisions
with the real world is very much open to question. If the status quo were
unbearable perhaps we’d be willing to experiment, but the status quo is fine.
Better than fine, in fact — people love driving. Wrestling people out of such
a habit would have to be done with _zero cooperation_ ; like the fingerprint
unlock, implanted silently into their normal behavior, rather than being
actively chosen.

The same applies to the grand threats of automation hanging over our heads.
Many people are under the impression that soon their job will either be done
by machine, or will become obsolete. While unquestionably this will be true in
some cases, the rule of _good enough_ suggests that it will be nowhere near as
prevalent as forecasts suggest.

Quite simply, in most cases where automation is possible, it would be too much
upheaval to make the marginal gains worthwhile. Automation would have to
overcome social and political opposition, demand a change in consumer
behavior, require years of iffy beta-testing, and for what? An improvement on
something that in most cases isn’t really in need of improvement in the first
place (so far as the market is concerned).

The moral of this story is a simple one. Just because a given technology is
possible and better doesn’t make it inevitable. The status quo must be
inadequate, or, at the very least, it should not require any upheaval of
behavior. These thresholds are rarely met by the majority of futurist
speculations capturing our imagination. This should worry us because we are
assuming a level of future growth that is by no means certain.

The futurists are unable to see an invisible wall standing before us, defined
by human limitations. The future — technologically speaking — will probably be
much more familiar than we imagine.
